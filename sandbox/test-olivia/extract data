import functions_framework
from google.cloud import secretmanager
from google.cloud import storage
import requests
from bs4 import BeautifulSoup
import pandas as pd
import datetime
import uuid

# settings
project_id = 'ba882-pipeline-olivia'
secret_id = 'motherduck'
version_id = 'latest'
bucket_name = 'ba882_olivia'


## store data to gcs

def upload_to_gcs(bucket_name, csv_data):
    """Uploads data to a Google Cloud Storage bucket as a CSV file."""
    client = storage.Client()
    bucket = client.bucket(bucket_name)
    
    # Get the current date in YYYYMMDD format
    current_date = datetime.datetime.now().strftime("%Y%m%d%H%M")
    
    # Set the GCS path to the format: data/YYYYMMDD/disease.csv
    blob_name = f"data/{current_date}/disease.csv"
    blob = bucket.blob(blob_name)

    # Upload the CSV data
    blob.upload_from_string(csv_data, content_type='text/csv')
    print(f"File {blob_name} uploaded to {bucket_name}.")

    return {'bucket_name': bucket_name, 'blob_name': blob_name}


# main task: extract data
def extract_data(year, week, disease_table):
    """Extracts CDC data from the given URL."""
    url = f'https://wonder.cdc.gov/nndss/static/{year}/{week:02d}/{year}-{week:02d}-table{disease_table:02d}.html'
    response = requests.get(url)

    if response.status_code == 200:
        soup = BeautifulSoup(response.content, 'html.parser')
        table = soup.find('table')

        # step 1: find the disease name
        disease_name = ""
        header_rows = table.find_all('tr')[:2]
        for header_row in header_rows:
            disease_header = header_row.find('th', colspan=True)
            if disease_header:
                disease_name += disease_header.get_text(strip=True) + " "
        disease_name = disease_name.strip()

        # store all the data in the list first
        data = []

        rows = table.find_all('tr')[2:]
        for row in rows:
            th = row.find('th')
            td = row.find_all('td')

            if th and len(td) >= 1:
                # step 2: get the reporting area
                region = th.get_text(strip=True)

                if 'Total' in region:
                    continue

                # step 3: get the weekly count
                current_week_data = td[0].get_text(strip=True)

                # Convert to numeric, handle errors by setting NaN to 0, and cast to integer
                current_week_data = pd.to_numeric(current_week_data, errors='coerce')
                if pd.isna(current_week_data):
                    current_week_data = 0
                current_week_data = int(current_week_data)

                # Combine year and week as 'Year_Week'
                year_week_dt = pd.to_datetime(f"{year}{str(week).zfill(2)}0", format='%Y%W%w')

                data.append({
                    "Disease": disease_name,
                    "Region": region,
                    "Current_week_count": current_week_data,
                    "Year_Week": year_week_dt
                })
        return data
    return None


@functions_framework.http
def task(request=None):
    # Instantiate the services 
    sm = secretmanager.SecretManagerServiceClient()
    storage_client = storage.Client()

    # Build the resource name of the secret version
    name = f"projects/{project_id}/secrets/{secret_id}/versions/{version_id}"

    # Access the secret version
    response = sm.access_secret_version(request={"name": name})
    md_token = response.payload.data.decode("UTF-8")





    start_year = 2023
    end_year = 2024
    weeks_range = range(1, 53)  
    disease_tables = [10,60,90,110,140,170,200,220,241,250,310,320,330,340,350,353,354,355,360,370,380,392,393,394,397,398,400,440,460,470,480,550,560,580,600,610,620,630,640,660,670,680,702,711,712,
                  714,715,732,741,742,744,745,748,770,773,774,776,777,790,800,811,812,860,890,900,907,920,930,940,950,960,968,970,971,990,1000,1010,1011,1020,1040,1050,1060,1090,1100,1110,1120,1121,1122,1129,
                  1130,1140,1141,1142,1250,1260,1270,1280,1290,1309,1310,1330,1340,1360,1382,1383,1390,1391,1292,1393,1394,1395,1396,1397,1398,1399,1400,1412]


    all_data = []

    for year in range(start_year, end_year + 1):
        for week in weeks_range:
            for disease_table in disease_tables:
                data = extract_data(year, week, disease_table)
                if data:
                    all_data.extend(data)
                    print(f"Successfully fetched data for Year {year}, Week {week}, Disease: {data[0]['Disease']}")

    if all_data:
        # Convert to DataFrame
        df = pd.DataFrame(all_data)

        # Convert DataFrame to CSV string
        csv_data = df.to_csv(index=False)

        # Upload CSV to GCS
        gcs_path = upload_to_gcs(bucket_name, csv_data)

        return {
            "num_entries": len(all_data),
            "bucket_name": gcs_path.get('bucket_name'),
            "blob_name": gcs_path.get('blob_name')
        }, 200

    else:
        return {"message": "No data found."}, 404
